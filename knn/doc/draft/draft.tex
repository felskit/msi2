\documentclass[11pt,a4paper]{article}
\usepackage{fullpage}
\usepackage[T1]{fontenc} 
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{tabularx}
\usepackage[hidelinks]{hyperref}
\usepackage[polish]{babel}

\def\arraystretch{1.5}
\renewcommand{\thesection}{\arabic{section}.}
\setlength{\parindent}{0cm}
\setlength{\parskip}{2mm}

\begin{document}

\title{Metody sztucznej inteligencji 2 \\
\Large{
    Projekt 1. --- Algorytm $k$ najbliższych sąsiadów \\
    Konspekt
}}
\author{Bartłomiej Dach, Tymon Felski}
\maketitle

\noindent
Niniejszy dokument zawiera informacje wstępne dotyczące pierwszego projektu, którego celem jest zaimplementowanie algorytmu $k$ najbliższych sąsiadów ($k$-NN, ang. \emph{$k$ nearest neighbors}) i~analiza jego działania dla~dostarczonych danych treningowych.

\section{Skład grupy}

Projekt realizowany będzie w~dwuosobowej grupie, w~składzie:

\begin{enumerate}
    \item Bartłomiej Dach,
    \item Tymon Felski.
\end{enumerate}

\section{Opis algorytmu}

Algorytm $k$ najbliższych sąsiadów jest jedną z~wielu metod rozwiązywania \textbf{problemu klasyfikacji}, czyli predykcji wartości zmiennych jakościowych (zwanych również zmiennymi kategorycznymi lub~dyskretnymi) na~podstawie przykładowych \textbf{danych treningowych} \cite[s.~9--10]{hastie2009}.
Klasycznym przykładem tego typu problemu jest klasyfikacja gatunków irysów z~użyciem pomiarów rozmiarów działek kielicha oraz płatków kwiatu.

Klasyfikatory oparte na~metodzie $k$ najbliższych sąsiadów operują na~punktach w~przestrzeni $n$-wymiarowej.
Proces klasyfikacji danego punktu $x_0 \in \mathbb{R}^n$ odbywa~się w~następujący sposób:

\begin{enumerate}
    \item Ze~zbioru treningowego wybierane jest $k$ punktów znajdujących~się najbliżej punktu $x_0$ \cite[s.~261]{vapnik1998}.
    Odległość między punktami definiowana jest najczęściej jako odległość euklidesowa w~przestrzeni $\mathbb{R}^n$.
    \item Jeżeli większość z~wybranych $k$ punktów należy do~jednej klasy, to~punkt wejściowy jest przypisywany do~tej samej klasy.
    \item Potencjalne remisy w~punkcie (2) są~rozstrzygane losowo \cite[s.~463--464]{hastie2009}.
\end{enumerate}

Liczba punktów wybieranych ze~zbioru treningowego (wartość~$k$) stanowi parametr algorytmu.
Jakość działania metody w~dużym stopniu zależy od~odpowiedniego doboru tego parametru dla~danego zadania klasyfikacji \cite[s.~468--470]{hastie2009}.

Ponieważ algorytm $k$-NN wymaga zapamiętania całości danych treningowych do~swojego działania, zaliczany jest on do~klasy klasyfikatorów \textbf{opartych na~pamięci} (ang. \emph{memory-based}) \cite[s.~463]{hastie2009}.

\section{Wybrane technologie}
Językiem programowania, który zdecydowano się wykorzystać, jest język skryptowy \textbf{Python} w~wersji 3.5.2.
Jest to~uwarunkowane między innymi sporymi możliwościami tego języka w~zakresie przetwarzania i~analizy danych, wygodą programowania i~przenośnością stworzonych rozwiązań.
W~celu uproszczenia pracy z~danymi oraz~ich analizy, wybrano także pomocnicze moduły Pythona, którymi są \textbf{pandas}, \textbf{NumPy} oraz \textbf{Matplotlib}.\\

\noindent
W~poniższej tabeli zestawiono wybrane biblioteki wraz z ich wersjami oraz określono licencje, na~których zostały udostępnione.
\begin{table}[H]
    \begin{tabularx}{\textwidth}{|c|l|X|l|c|}
        \hline
        \textbf{Nr} & \textbf{Komponent, wersja} & \textbf{Opis} & \textbf{Licencja} & \\
        \hline
        \hline
        1 & 
        Matplotlib, 2.1.0 & 
        Umożliwia tworzenie wykresów &
        Matplotlib License &
        \cite{matplotlib} \\
        \hline
        2 & 
        NumPy, 1.13.3 &
        Używana do~efektywnych obliczeń na~wektorach $n$-wymiarowych &
        BSD License &
        \cite{numpy} \\
        \hline
        3 &
        pandas, 0.21.0 &
        Wspomaga ładowanie danych z~plików CSV oraz~ich analizę &
        BSD License &
        \cite{pandas} \\
        \hline
    \end{tabularx}
    \caption{Wykorzystane biblioteki wraz z~określeniem licencji}
\end{table}

\begin{thebibliography}{9}

    \bibitem{matplotlib}
        Matplotlib Development Team:
        Matplotlib.
        Oficjalna strona: \url{https://matplotlib.org}.
        [Dostęp 26~lutego 2018]

    \bibitem{pandas}
        McKinney W.:
        pandas -- Python Data Analysis Library.
        Oficjalna strona: \url{https://pandas.pydata.org}.
        [Dostęp 26~lutego 2018]

    \bibitem{numpy}
        Oliphant T.:
        NumPy.
        Oficjalna strona: \url{http://www.numpy.org}.
        [Dostęp 26~lutego 2018]

    \bibitem{hastie2009}
        T. Hastie,
        R. Tibshirani,
        J. Friedman,
        \emph{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}.
        Nowy Jork: Springer-Verlag,
        2009.
        [Online] \\
        Dostępne: \url{https://web.stanford.edu/~hastie/ElemStatLearn/}.
        [Dostęp 26~lutego 2018]

    \bibitem{vapnik1998}
        V.N. Vapnik,
        \emph{Statistical learning theory}.
        Nowy Jork: John Wiley and Sons,
        1998.

\end{thebibliography}

\end{document}