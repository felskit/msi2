\documentclass[11pt,a4paper]{article}
\usepackage{fullpage}
\usepackage[T1]{fontenc} 
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage[table,dvipsnames]{xcolor}
\usepackage[hidelinks]{hyperref}
\usepackage[polish]{babel}

\def\arraystretch{1.5}
\setlength{\parindent}{0cm}
\setlength{\parskip}{2mm}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

\begin{document}

\title{Metody sztucznej inteligencji 2 \\
\Large{
    Projekt 1. --- Algorytm $k$ najbliższych sąsiadów \\
    Raport
}}
\author{Bartłomiej Dach, Tymon Felski}
\maketitle

Niniejszy dokument zawiera raport z~implementacji algorytmu $k$ najbliższych sąsiadów ($k$-NN, ang. \emph{$k$ nearest neighbors}) oraz~analizę efektywności jego działania dla~dostarczonych danych treningowych.

\section{Algorytm $k$ najbliższych sąsiadów}

Algorytm $k$ najbliższych sąsiadów jest jedną z~wielu metod rozwiązywania \textbf{problemu klasyfikacji}, czyli predykcji wartości zmiennych jakościowych (zwanych również zmiennymi kategorycznymi lub~dyskretnymi) na~podstawie przykładowych \textbf{danych treningowych} \cite[s.~9--10]{hastie2009}.
Klasycznym przykładem tego typu problemu jest klasyfikacja gatunków irysów z~użyciem pomiarów rozmiarów działek kielicha oraz płatków kwiatu.

Klasyfikatory oparte na~metodzie $k$ najbliższych sąsiadów operują na~punktach w~przestrzeni $n$-wymiarowej.
Proces klasyfikacji danego punktu $x_0 \in \mathbb{R}^n$ odbywa~się w~następujący sposób:

\begin{enumerate}
    \item Ze~zbioru treningowego wybierane jest $k$ punktów znajdujących~się najbliżej punktu $x_0$ \cite[s.~261]{vapnik1998}.
    \item Jeżeli większość z~wybranych $k$ punktów należy do~jednej klasy, to~punkt wejściowy jest przypisywany do~tej samej klasy.
    \item Potencjalne remisy w~punkcie (2) są~rozstrzygane losowo \cite[s.~463--464]{hastie2009}.
\end{enumerate}

Liczba punktów wybieranych ze~zbioru treningowego (wartość~$k$) stanowi parametr algorytmu.
Jakość działania metody w~dużym stopniu zależy od~odpowiedniego doboru tego parametru dla~danego zadania klasyfikacji \cite[s.~468--470]{hastie2009}.

Do~parametrów algorytmu można zaliczyć również metrykę używaną do~obliczania odległości między punktami.
W~zaimplementowanym algorytmie uwzględnione zostały następujące metryki:

\begin{enumerate}
    \item \textbf{odległość euklidesowa}:
    $$ d(x,y) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2} $$
    \item \textbf{odległość taksówkowa (miejska, Manhattan)}:
    $$ d(x,y) = \sum_{i=1}^n |x_i - y_i| $$
    \item \textbf{odległość Czebyszewa (maksimum)}:
    $$ d(x,y) = \max_{i=1,\dots,n} |x_i - y_i| $$
    \item \textbf{odległość Minkowskiego} z~parametrem~$p$, stanowiąca uogólnienie powyższych:
    $$ d(x,y) = \left( \sum_{i=1}^n |x_i - y_i|^p \right)^{1/p} $$
\end{enumerate}

Ponieważ algorytm $k$-NN wymaga zapamiętania całości danych treningowych do~swojego działania, zaliczany jest on do~klasy klasyfikatorów \textbf{opartych na~pamięci} (ang. \emph{memory-based}) \cite[s.~463]{hastie2009}.

\section{Sposób analizy efektywności algorytmu}

Efektywność działania $k$-NN była mierzona z~użyciem dwóch dostarczonych dwuwymiarowych zbiorów danych oraz~zastosowaniem metody kroswalidacji, zalecanej do~doboru parametrów metody \cite[s.~470]{hastie2009}.

\subsection{Kroswalidacja}

\textbf{$k$-krotna kroswalidacja (walidacja krzyżowa, sprawdzian krzyżowy)} to~technika ewaluacji metod klasyfikacji z~użyciem zebranych danych.
W~tej technice zbiór danych treningowych $X$ dzielony~jest losowo na~$k$ rozłącznych podzbiorów $X_1,\dots,X_k$ podobnej liczności.

Po~podziale danych wykonywane jest $k$ serii testowych.
W $i$-tej serii testowany klasyfikator jako~dane treningowe otrzymuje zbiór $X \setminus X_i$.
Zadaniem klasyfikatora jest wyznaczenie klas dla~punktów ze~zbioru $X_i$.
Następnie dla~każdego punktu $x_j \in X_i$ następuje porównanie etykiety $y'_j$ wyznaczonej przez testowaną metodę z~etykietą faktyczną $y_j$ \cite[s.~241--243]{hastie2009}.

Główną metryką dokładności algorytmu przy~tym porównaniu jest \textbf{proporcja błędnej klasyfikacji}, obliczana wzorem
$$ e_i = \frac{1}{|X_i|} \cdot | \{ y_j \neq y'_j : j = 1,2,\dots,|X_i| \} | $$
Końcowa proporcja błędnej klasyfikacji dla~danej metody jest obliczane poprzez uśrednienie uzyskanych $k$ wyników.

\subsection{Rozważane zbiory treningowe}

Algorytm został przetestowany na~dwuwymiarowych zbiorach {\tt simple} i {\tt three\_gauss}, w~których większość punktów zawiera~się w~kwadracie $[-1,1] \times [-1,1]$.
Oba zbiory mają dość regularną charakterystykę, dzięki czemu możliwe jest również zastosowanie metod statystycznych do~oceny efektywności klasyfikatora $k$-NN.

\subsubsection{Zbiór {\tt simple}}

Zbiór {\tt simple} składa~się z~punktów o~rozkładzie zbliżonym do~jednostajnego wzdłuż obu współrzędnych. % [CITATION NEEDED]
Punkty w~tym zbiorze podzielone~są na~dwie klasy wzdłuż prostej o~równaniu $x=y$.
Docelowo klasyfikator powinien więc jak najwierniej odwzorować podział punktów wzdłuż tej prostej.

\subsubsection{Zbiór {\tt three\_gauss}}

Zbiór {\tt three\_gauss} zawiera trzy klasy, częściowo nakładające~się na~siebie.
Każda z~trzech klas charakteryzuje~się rozkładem podobnym do~dwuwymiarowego rozkładu normalnego:

$$ f(\mathbf{x}) = \frac{\exp \left( -\frac{1}{2} (\mathbf{x} - \boldsymbol\mu)^\mathsf{T} \boldsymbol\Sigma^{-1} (\mathbf{x} - \boldsymbol\mu) \right)}{2\pi \sqrt{|\boldsymbol\Sigma|}} $$

Na~podstawie zbioru zawierającego 30~000 punktów wyznaczone zostały przybliżone parametry rozkładów:

\begin{enumerate}
    \item Dla~klasy~1. mamy
    $$
        \boldsymbol\mu_1 \approx \begin{bmatrix} 0.1 \\ 0.5 \end{bmatrix},
        \qquad
        \boldsymbol\Sigma_1 \approx \begin{bmatrix} 0.04 & 0 \\ 0 & 0.04 \end{bmatrix},
    $$
    \item Dla~klasy~2. mamy
    $$
        \boldsymbol\mu_2 \approx \begin{bmatrix} -0.6 \\ 0.2 \end{bmatrix},
        \qquad
        \boldsymbol\Sigma_2 \approx \begin{bmatrix} 0.04 & 0 \\ 0 & 0.0025 \end{bmatrix},
    $$
    \item Dla~klasy~3. mamy
    $$
        \boldsymbol\mu_3 \approx \begin{bmatrix} 0 \\ -0.3 \end{bmatrix},
        \qquad
        \boldsymbol\Sigma_3 \approx \begin{bmatrix} 0.03 & -0.045 \\ -0.045 & 0.16 \end{bmatrix}.
    $$
\end{enumerate}

% TODO: policzyć wartość oczekiwaną, macierz kowariancji

\section{Wyniki eksperymentów}

Zgodnie z opisem zawartym w poprzednim rozdziale, jakość klasyfikacji przy pomocy algorytmu $k$-NN została przetestowana metodą kroswalidacji przy użyciu dwóch zbiorów treningowych i~różnych wartości parametrów. Zbadano zbiory {\tt simple} oraz {\tt three\_gauss} zawierające odpowiednio $1000$ i $3000$ elementów. Pod uwagę wzięto wartości parametru $k$ ze zbioru $\{1, 3, 5, 7, 9, 11, 13\}$ oraz pięć metryk:
\begin{itemize}
    \setlength\itemsep{-.4em}
    \item odległość taksówkową,
    \item odległość euklidesową,
    \item odległość Czebyszewa,
    \item odległość Minkowskiego z parametrem $p$ wynoszącym $1.5$,
    \item odległość Minkowskiego z parametrem $p$ wynoszącym $3$.
\end{itemize}
Zastosowano pięciokrotną kroswalidację, tzn. dane treningowe zostały losowo podzielone na~$5$~rozłącznych podzbiorów. % TODO: coś o seedzie?

\subsection{Zbiór {\tt simple}}

Wyniki testów jakości klasyfikacji przy pomocy pięciokrotnej kroswalidacji dla zbioru {\tt simple} znajdują się w tabeli poniżej. % TODO: opisać czym jest ten ułamek (czego stosunek)
Kolorem zielonym zaznaczono wynik najlepszy, a kolorem czerwonym -- najgorszy. Poza najlepszym i najgorszym wynikiem, na pomarańczowo wyróżniono także dwa zestawy parametrów z wynikiem bardzo zbliżonym do najlepszego.

\begin{table}[H]
    \begin{tabularx}{\textwidth}{|c|c|Y|Y|Y|Y|Y|Y|Y|}
        \cline{3-9}
        \multicolumn{2}{c|}{} & \multicolumn{7}{c|}{\textbf{Wartość parametru $k$}} \\
        \cline{3-9}
        \multicolumn{2}{c|}{} & 1 & 3 & 5 & 7 & 9 & 11 & 13 \\
        \hline
        \multirow{5}{*}{\rotatebox[origin=c]{90}{\textbf{Użyta metryka}}}
        & taksówkowa & 0.9850 & \cellcolor{SpringGreen!50}0.9880 &  \cellcolor{YellowOrange!30}0.9870 & 0.9800 & 0.9820 & 0.9800 & 0.9810 \\
        \cline{2-9}
        & euklidesowa & 0.9850 & 0.9860 & 0.9820 & 0.9820 & 0.9830 & 0.9830 & \cellcolor{BrickRed!30}0.9790 \\
        \cline{2-9}
        & Czebyszewa & 0.9830 & 0.9860 & 0.9820 & 0.9810 & 0.9800 & 0.9810 & 0.9820 \\
        \cline{2-9}
        & Minkowskiego ($1.5$) & 0.9850 &  \cellcolor{YellowOrange!30}0.9870 & 0.9840 & 0.9800 & 0.9830 & 0.9810 & 0.9800 \\
        \cline{2-9}
        & Minkowskiego ($3$) & 0.9840 & 0.9850 & 0.9840 & 0.9820 & 0.9800 & 0.9810 & 0.9800 \\
        \hline
    \end{tabularx}
    \caption{Wyniki kroswalidacji algorytmu $k$-NN dla zbioru treningowego {\tt simple}}
    \label{tab:simple-all}
\end{table}

% TODO: skomentować ogólną tendencję (małe k lepsze, duże k gorsze, które metryki dały podobne wyniki do najlepszej i wcale nie były aż tak bardzo gorsze)

Po ustaleniu najkorzystniejszych wartości parametrów algorytmu $k$-NN dla powyższego zbioru, zdecydowano się zbadać cały obszar $[-1,1] \times [-1,1]$ z gęstością próbkowania $\delta = 0.05$ i ustalić przynależność środkowych punktów każdego z kwadratów o boku~$\delta$, zawartych w tym obszarze, do jednej z klas zbioru treningowego. Poniższy wykres jest graficzną reprezentacją otrzymanych wyników dla $k = 3$ oraz metryki będącej odłegłością taksówkową.

\begin{figure}[H]
  \centering
  \includegraphics[width=.8\textwidth]{res/simple-manhattan-3.pdf}
  \caption{Klasyfikacja punktów z obszaru $[-1,1] \times [-1,1]$ dla $k = 3$ i odległości taksówkowej}
  \label{fig:simple-manhattan-3}
\end{figure}

Dla kontrastu, podobną analizę przeprowadzono dla parametrów, które zapewniły najgorszą klasyfikację, czyli dla $k=13$ i odległości euklidesowej.

\begin{figure}[H]
  \centering
  \includegraphics[width=.8\textwidth]{res/simple-euclidean-13.pdf}
  \caption{Klasyfikacja punktów z obszaru $[-1,1] \times [-1,1]$ dla $k = 13$ i odległości euklidesowej}
  \label{fig:simple-euclidean-13}
\end{figure}

% TODO: skomentować drobne różnice w kolorowaniu

\subsection{Zbiór {\tt three\_gauss}}

Pięciokrotną kroswalidację badanego klasyfikatora przeprowadzono także dla zbioru {\tt three\_gauss}. Jej wyniki znajdują się w tabeli poniżej. Komórki tabeli wyróżnione kolorami: zielonym, czerwonym i pomarańczowym mają tutaj takie same znaczenia.

\begin{table}[H]
    \begin{tabularx}{\textwidth}{|c|c|Y|Y|Y|Y|Y|Y|Y|}
        \cline{3-9}
        \multicolumn{2}{c|}{} & \multicolumn{7}{c|}{\textbf{Wartość parametru $k$}} \\
        \cline{3-9}
        \multicolumn{2}{c|}{} & 1 & 3 & 5 & 7 & 9 & 11 & 13 \\
        \hline
        \multirow{5}{*}{\rotatebox[origin=c]{90}{\textbf{Użyta metryka}}}
        & taksówkowa & \cellcolor{BrickRed!30}0.9053 & 0.9233 & 0.9257 & 0.9283 & \cellcolor{SpringGreen!50}0.9333 & 0.9280 & 0.9290 \\
        \cline{2-9}
        & euklidesowa & 0.9073 & 0.9220 & 0.9230 & 0.9303 & 0.9290 & 0.9290 & 0.9267 \\
        \cline{2-9}
        & Czebyszewa & 0.9113 & 0.9213 & 0.9217 & 0.9253 & 0.9277 & \cellcolor{YellowOrange!30}0.9307 & 0.9300 \\
        \cline{2-9}
        & Minkowskiego ($1.5$) & 0.9070 & 0.9237 & 0.9253 & 0.9290 & \cellcolor{YellowOrange!30}0.9307 & 0.9280 & 0.9263 \\
        \cline{2-9}
        & Minkowskiego ($3$) & 0.9090 & 0.9210 & 0.9230 & 0.9300 & 0.9297 & 0.9297 & 0.9297 \\
        \hline
    \end{tabularx}
    \caption{Wyniki kroswalidacji algorytmu $k$-NN dla zbioru treningowego {\tt three\_gauss}}
    \label{tab:simple-all}
\end{table}

% TODO: skomentować ogólną tendencję (duże k lepsze, małe k gorsze, które metryki dały podobne wyniki do najlepszej i wcale nie były aż tak bardzo gorsze)

Dla wyróżnionych parametrów algorytmu $k$-NN podczas klasyfikacji zbioru {\tt three\_gauss} przeprowadzono analogiczną analizę, jak w przypadku zbioru {\tt simple}. Punkty z obszaru $[-1,1] \times [-1,1]$ z gęstością próbkowania $\delta = 0.05$ zostały sklasyfikowane, w celu wizualizacji przyporządkowania. Poniższy wykres zawiera wyniki uzyskane dla wywołania algorytmu $k$-NN dla $k=9$ i odległości taksówkowej.

\begin{figure}[H]
  \centering
  \includegraphics[width=.8\textwidth]{res/gauss-manhattan-9.pdf}
  \caption{Klasyfikacja punktów z obszaru $[-1,1] \times [-1,1]$ dla $k = 9$ i odległości taksówkowej}
  \label{fig:gauss-manhattan-9}
\end{figure}

Podobnie jak w poprzednim podrozdziale, dla zbioru {\tt three\_gauss} również zwizualizowano wynik działania algorytmu $k$-NN dla najgorszego zestawu parametrów, czyli $k=1$ i odległości taksówkowej. Przedstawia to poniższy wykres.

\begin{figure}[H]
  \centering
  \includegraphics[width=.8\textwidth]{res/gauss-manhattan-1.pdf}
  \caption{Klasyfikacja punktów z obszaru $[-1,1] \times [-1,1]$ dla $k = 1$ i odległości taksówkowej}
  \label{fig:gauss-manhattan-1}
\end{figure}

% TODO: skomentować ogromne różnice w kolorowaniu

\section{Wnioski}

% TODO: zestawić najlepsze/najgorsze w każdym i przedstawić jakąś propozycję (wysnuć wniosek jakie parametry lepsze w jakim przypadku i dlaczego), napisać że to było przewidywalne, wymyślić jeszcze jakieś kryterium?

\begin{thebibliography}{9}

    \bibitem{hastie2009}
        T. Hastie,
        R. Tibshirani,
        J. Friedman,
        \emph{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}.
        Nowy Jork: Springer-Verlag,
        2009.
        [Online] \\
        Dostępne: \url{https://web.stanford.edu/~hastie/ElemStatLearn/}.
        [Dostęp 26~lutego 2018]

    \bibitem{vapnik1998}
        V.N. Vapnik,
        \emph{Statistical learning theory}.
        Nowy Jork: John Wiley and Sons,
        1998.

\end{thebibliography}

\end{document}